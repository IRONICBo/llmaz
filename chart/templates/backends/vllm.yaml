{{- if .Values.backendRuntime.install -}}
apiVersion: inference.llmaz.io/v1alpha1
kind: BackendRuntime
metadata:
  labels:
    app.kubernetes.io/name: backendruntime
    app.kubernetes.io/part-of: llmaz
    app.kubernetes.io/created-by: llmaz
  name: vllm
spec:
  commands:
    - python3
    - -m
    - vllm.entrypoints.openai.api_server
  image: vllm/vllm-openai
  version: v0.6.0
  # Do not edit the preset argument name unless you know what you're doing.
  # Free to add more arguments with your requirements.
  args:
    - name: default
      flags:
        - --model
        - "{{`{{ .ModelPath }}`}}"
        - --served-model-name
        - "{{`{{ .ModelName }}`}}"
        - --host
        - "0.0.0.0"
        - --port
        - "8080"
    - name: speculative-decoding
      flags:
        - --model
        - "{{`{{ .ModelPath }}`}}"
        - --served-model-name
        - "{{`{{ .ModelName }}`}}"
        - --speculative_model
        - "{{`{{ .DraftModelPath }}`}}"
        - --host
        - "0.0.0.0"
        - --port
        - "8080"
        - --use-v2-block-manager
        - --num_speculative_tokens
        - "5"
        - -tp
        - "1"
  resources:
    requests:
      cpu: 4
      memory: 8Gi
    limits:
      cpu: 4
      memory: 8Gi
{{- end }}
