# This is just an toy example, because it doesn't make any sense
# in real world, drafting tokens for the model with similar size.

apiVersion: inference.llmaz.io/v1alpha1
kind: Playground
metadata:
  name: llamacpp-speculator
spec:
  replicas: 1
  modelClaims:
    models:
      - name: llama2-7b-q8-gguf # the target model
        role: main
      - name: llama2-7b-q2-k-gguf  # the draft model
        role: draft
  backendRuntimeConfig:
    name: llamacpp
    argFlags:
      - -fa # use flash attention
    resources:
      requests:
        cpu: 4
        memory: "8Gi"
      limits:
        cpu: 4
        memory: "8Gi"
